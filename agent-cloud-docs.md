# Agent Cloud (Project Merlin)

*Build a data-to-insight machine that can deliver improved returns, institutional grade tracking, and performance reporting ‚Äî all run on low cost, scalable, managed cloud infrastructure. With the latest AI models integrated to remove toil, provide contextual analysis, and highlight insights.*

*Our OODA Loop: Research ‚á¢ Company Selection. Portfolio Construction ‚á¢ Refinement ‚á¢ Deployment ‚á¢ Signals Collection ‚á¢ Sense Making ‚á¢ Decisioning*

*We seek to generate alpha in every one of those steps. To do that, we need to be in the top 10% of our peers at step of our process.*

**Sections:**

[Overview](#overview)\
[Three Phases](#three-phases)\
[System Design](#system-design)\
[Infrastructure](#infrastructure)\
[APIs (Inputs)](#apis)\
[Messages (Outputs)](#rich-messages)\
[Appendix](#appendix)\
[Database Fields](#database-fields)
<br/><br/>

***
<a name="overview"></a>
### Overview

For the last 3 years, I've been developing a set of thematic public equity funds called [Primal Funds](https://www.primalfunds.com). They're effectively small baskets of publicly traded companies where alpha is generated by a) company selection, b) the weight of each company in the portfolio, c) disciplined trading and rebalancing, and d) our proprietary research and investment process. Agent Cloud, originally called Project Merlin, is designed to improve each of these by combining real-time data with AI analysis.

This work is meant to be foundational ‚Äî a rough Version 1.0 from which we can iterate and build. It's about building the foundational serverless infrastructure needed to power future iterations of our agents, while shipping an initial set of services that improve our daily operations starting today.

So what are we building?

**TDLR: I'd like to create the serverless infrastructure to ingest data from a number of sources (e.g. Databento, Polygon.io, Interactive Brokers, SEC EDGAR 13Fs) roughly once per day, store them in S3 and Aurora/DyanmoDB, and then run a number of scheduled jobs (daily, weekly, monthly, quarterly, 1H/2H, and annually) that format that data into markdown-formatted messages delivered via Slack that include AI analysis (contextually relevant and insightful observations) and even AI audio readouts.**

*This project is inspired by Philippe Page's writing on the [Agent Cloud](https://blog.mainfra.me/p/agent-clouds) as a futuristic idea.*
<br/><br/>

***
<a name="three-phases"></a>
### Three Phases
To accomplish the goal set above, I'd like to approach the project in three separate phases:

**Phase 1: Establish Golden Records**<br/>
Setup all of the serverless infrastructure and API connections to begin ingesting a range of data at least once per day to create immaculate (golden/master) records for:
*  **Fund Models:** Portfolios that include a name, base currency, positions (ticker symbols), % weight for each position, a benchmark to compare performance against (e.g. Nasdaq 100), as well as daily % change, with version numbering and/or a full changelog or archive so we have historical data on all updates (as these change as often as once per month). Ideally these would be easily updatable by me via S3 using a JSON file if possible. All of this data will be provided by me (via S3 JSON) but will need to be matched against each model in Interactive Brokers and de-duplicated/matched so we have one source of truth from two data sets.
* **Current Holdings:** An exhaustive list of all positions from all current model portfolios (as well as Interactive Brokers data, which can have other one-off positions) that includes deep context on each security ‚Äî including ticker symbol, currency, exchange, issuer/company name, daily open/close price, and more. These do not need to be easily updatable by me, as long as they reflect every position in every model fund and in Interactive Brokers. What will be important is gracefully updated this to reflect all of those positions at least once at the end of each day. All of this data will come from either Databento's or Factset's API.
* **Benchmarks:** Each of our model portfolios has a benchmark (e.g. S&P 500, Nasdaq 100, MSCI World) that we track performance against. So we'll also need a database of daily open/close prices and daily % change for all benchmarks. That list of benchmarks can be set once and likely never change.
* **Market Closure Schedule:** This may be solvable via Databento's or Factset's API, but given that we'll be ingesting market data and reporting out on it at least once daily I think it would be helpful to store our own market closure data. This would allow us to know when markets are closed to avoid pinging APIs and send out simple "XXXX markets are closed today" reports. We will likely need this for all of our major exchanges/markets including NYSE/Nasdaq/DJIA, Canada, Mexico, London, Paris, Italy, Netherlands, Sweden, Switzerland, Israel, Japan, and China. Once we've gathered this data, I'm not sure how often it would need to be updated ‚Äî if ever.

	**The following are lower priority and can be done as part of Phase 3 instead:**
* **13F Entities & Holdings:** There are two parts to this one. The first is another (ideally) JSON file editable by me stored on S3 that includes a list of 13F entities (legal entity, common name, website(s), and Chief Investment Officer) for which I would like to pull 13F filing data when it's published each quarter. The second part will be a database that includes all holdings disclosed by each filing entity each quarter ‚Äî which will be analyzed via AI LLM and then reported out in a pre-formatted message via Slack (with a backup of that message stored in S3 for long-term cold storage).
* **University & Charity Endowments:** Similar to 13Fs, this will also include an editable file where I can add or update a list of endowments (their name, website(s), and Chief Investment Officer). Then, once setup, a scheduled job will run once per year in late October (likely the 30th) to grab search results from Perplexity (in the future potentially 2-3 models) on each endowment's latest returns, commentary, and annual report. (This could also be broadened out over time to include other key investment firms that publish quarterly letters, where we would want the same job done. The goal here is to minimize manual checking of these reports and have them pushed versus pulled.) All of this would be reported out in a pre-formatted message via Slack with a backup stores in long-term cold storage in S3.
* **Alert Subjects:** Similarly again, this would include an editable file where I can add/update a list of "subjects" (a mix of people, company names, and search strings/terms) to create my own version of Google Alerts. Then each week a scheduled job would ask Perplexity (in the future likely 2-3 other models) to search the web, YouTube, Reddit, X, and other sources for updates on those terms. This report would then be sent in a pre-formatted message via Slack and likely not have a backup stored in S3.

**Phase 2: Performance Reports**<br/>
Using all of the models, holdings, and benchmark data collected and stored in Phase 1 we can now begin to a) format that data into pre-formatted messages that will be sent via Slack's API, b) compute statistics (e.g. daily return of computer model vs real-world model in Interactive Brokers), c) request and include AI analysis done by 1 or more models, and d) send that data via Slack's API to separate channels in our shared workspace. These performance reports will each look slightly differently as we want to emphasize different data depending on the time frequency of the reports. But these reports will include:
*  **Daily Report:** Sent daily <15 mins after market close (4 PM Eastern).
*  **Weekly Report:** Sent Saturday morning at 8 AM.
*  **Monthly Report:** Sent on the first day of each new month at 8 AM.
*  **Quarterly Report:** Sent on the first day of each new quarter at 8 AM.
*  **1H/2H Report:** Sent every July 1st and January 1st at 8 AM.
*  **Annual Report:** Sent on January 1st each year at 8 AM.<br/>

	While I am still fleshing out the details for the quarterly, 1H/2H, and annual reports, you can find the Version 1.0 template for the daily report below in the [Messages (Outputs)](#rich-messages) section. Most of the statistics can be computed in Python from data stored in Phase 1. As a backup, we can also grab statistic data from APIs like Databento or find a financial statistics library.

**Phase 3: Signal Reports**<br/>
In this last phase, we would tackle what I'm called "Signal Reports" which are the weekly reports on subjects of interests (people, companies, search strings/terms similar to Google Alerts), quarterly reports on things like the latest 13F filings, and annual reports on things like the University & Charity Endowment reports. All of these will combine some form of a) 1 or more AI model query (starting with Perplexity), b) potential additional AI model analysis and insight generation (e.g. turn endowment report PDF into LLM readable format and ask for analysis using agent with role and instructions), c) pre-formatted rich markdown messages containing the final analysis, and d) sending these reports via Slack's API to separate channels within a shared workspace. More on each of these reports:
*  **Weekly "Subject of Interest" Reports:** This will combine elements A-D mentioned above to do a search for the Subjects of Interest (specified in Phase 1 in the editable file stored in S3) to check for updates that ideally haven't been shared in previous weeks (omitting previous results) from a broad search across the web including YouTube, X, Reddit, podcasts, and other sources. With a simple Perplexity search, I typically get better results than I do through Google Alerts. So the goal here is to build on Perplexity as a baseline and add other AI model outputs (Gemini for YouTube and search?) and add rigor (use custom agent with history of previous queries to de-duplicate previously shared results?) to those final results and analysis.
*  **Quarterly 13F Reports:** This will combine elements A-D mentioned above to do a search for each 13F filers (specified in Phase 1 in the editable file stored in S3) latest 13F and report out on the latest holdings (via list), changes in holdings from the last quarter (increases, trims, and sellouts), with as much contextual analysis done by AI as possible. It should also include a link to the 13F filing with the SEC if possible.
*  **Annual Endowment Reports:** This will combine elements A-D mentioned above to do a search for each university or charity endowment (specified in Phase 1 in the editable file stored in S3) to share as detailed a recap of the performance, commentary, and investments over the last year, along with links (whenever possible) to the press release for the endowment report and ideally a PDF copy of the report (when shared).
<br/><br/>

***
<a name="infrastructure"></a>
### Infrastructure

Requirement: low cost, scalable, managed serverless infrastructure from AWS. Includes:

* [AWS Lambda - Serverless Compute](https://aws.amazon.com/pm/lambda/)
* [Amazon Simple Storage Service (S3) - Long-Term Storage](https://aws.amazon.com/s3/)
* [Amazon Aurora - Storage of Daily Pricing & Performance](https://aws.amazon.com/rds/aurora/)
* [Amazon EC2 - IBKR Gateway](https://aws.amazon.com/ec2/)
* [Amazon EventBridge - Scheduled Jobs & Events](https://aws.amazon.com/eventbridge/)
* [Amazon Simple Queue Service (SQS) - Queueing of Jobs](https://aws.amazon.com/pm/sqs/)
* [AWS Secrets Manager - Credential Security](https://aws.amazon.com/secrets-manager/)
* [AWS Glue - Data Movement](https://aws.amazon.com/glue/)
* [Amazon Bedrock - Antrophic's Claude and Other AI Models](https://aws.amazon.com/bedrock/)
<br/><br/>

***
<a name="apis"></a>
### APIs (Inputs)

Proprietary signals, synthesis, and analysis tools via APIs. Includes:

* [Interactive Brokers - IB Gateway APIs - Account Value & Model Performance Data](https://www.interactivebrokers.com/campus/ibkr-api-page/ibkr-api-home/)
* [Databento Reference API - Security Pricing & Enrichment](https://databento.com/docs/api-reference-reference?historical=python&live=python&reference=python)
* [Polygon API - Index Pricing & Enrichment](https://github.com/polygon-io/client-python)
* [FactSet API - Proprietary Signals](https://developer.factset.com/api-catalog)
* [Anthropic Claude in Amazon Bedrock - Analysis](https://aws.amazon.com/bedrock/claude/)
* [Perplexity API - Web Search](https://docs.perplexity.ai/home)
* [OpenAI API - Analysis](https://openai.com/api/)
* [Eleven Labs API - Voice](https://elevenlabs.io/api)
* [Slack API - Send Reports to Channels in Workspace](https://api.slack.com/methods/chat.postMessage)
<br/><br/>

***
<a name="rich-messages"></a>
### Rich Messages (Outputs)

*These will be delivered through Slack's API to specific channels (#daily-performance, #monthly-performance, #13Fs, #subjects-of-interest) in a single workspace.*

Requirements:
* Use Slack's Block Kit ([https://api.slack.com/block-kit/building](https://api.slack.com/block-kit/building))
* Rich text formatting ([Slack's Block Kit](https://api.slack.com/reference/block-kit/blocks#rich_text))
* Do not unfurl links
* Use File block to render file previews
<br/><br/>

**Template for Daily Performance Report:**

```
Daily performance report for Month XX, XXXX.

Aggregate Portfolio +/- XX.XX%

Benchmarks:
* S&P 500 +/- XX.XX%
* Nasdaq 100 +/- XX.XX%
* MSCI AWI +/- XX.XX%

Top 5 Holdings:
* XXXX Company Name +/- XX.XX%
* XXXX Company Name +/- XX.XX%
* XXXX Company Name +/- XX.XX%
* XXXX Company Name +/- XX.XX%
* XXXX Company Name +/- XX.XX%

Bottom 5 Holdings:
* XXXX Company Name +/- XX.XX%
* XXXX Company Name +/- XX.XX%
* XXXX Company Name +/- XX.XX%
* XXXX Company Name +/- XX.XX%
* XXXX Company Name +/- XX.XX%

Models:
* Model Name +/- XX.XX% (+/- XX.XX% Tracking Error)
* Model Name +/- XX.XX% (+/- XX.XX% Tracking Error)
* Model Name +/- XX.XX% (+/- XX.XX% Tracking Error)
* Model Name +/- XX.XX% (+/- XX.XX% Tracking Error)
* Model Name +/- XX.XX% (+/- XX.XX% Tracking Error)
* Model Name +/- XX.XX% (+/- XX.XX% Tracking Error)
* Model Name +/- XX.XX% (+/- XX.XX% Tracking Error)
* Model Name +/- XX.XX% (+/- XX.XX% Tracking Error)
* Model Name +/- XX.XX% (+/- XX.XX% Tracking Error)
* Model Name +/- XX.XX% (+/- XX.XX% Tracking Error)
* Model Name +/- XX.XX% (+/- XX.XX% Tracking Error)
* Model Name +/- XX.XX% (+/- XX.XX% Tracking Error)
* Model Name +/- XX.XX% (+/- XX.XX% Tracking Error)
* Model Name +/- XX.XX% (+/- XX.XX% Tracking Error)
* Model Name +/- XX.XX% (+/- XX.XX% Tracking Error)
* Model Name +/- XX.XX% (+/- XX.XX% Tracking Error)
* Model Name +/- XX.XX% (+/- XX.XX% Tracking Error)
* Model Name +/- XX.XX% (+/- XX.XX% Tracking Error)
* Model Name +/- XX.XX% (+/- XX.XX% Tracking Error)
* Model Name +/- XX.XX% (+/- XX.XX% Tracking Error)

Analysis:
1-2 paragraphs on what stands out in terms of aggregate vs benchmarks, top 5 and bottom 5 holdings, and model performance. All generated via Anthropic's Claude.

Top Stories:
* [Story Headline](https://www.factset.com)
* [Story Headline](https://www.factset.com)
* [Story Headline](https://www.factset.com)
* [Story Headline](https://www.factset.com)
* [Story Headline](https://www.factset.com)
* [Story Headline](https://www.factset.com)
* [Story Headline](https://www.factset.com)
* [Story Headline](https://www.factset.com)
* [Story Headline](https://www.factset.com)
* [Story Headline](https://www.factset.com)

Company Actions:
* [LVMH - Q4 Earnings](https://www.factset.com)
```
<br/>

**Monthly, quarterly, one-half, and annual performance reports:**

```

```
<br/>

***
<a name="appendix"></a>
### Appendix

Documentation on IBKR Gateway setup:
* VERY IMPORTANT Automation for IBKR Gateway: [https://github.com/IbcAlpha/IBC](https://github.com/IbcAlpha/IBC) - Latest Release: [https://github.com/IbcAlpha/IBC/releases/tag/3.20.0](https://github.com/IbcAlpha/IBC/releases/tag/3.20.0)
* Most recent guide to setup in AWS: [https://wasteofserver.com/interactive-brokers-tws-gateway-api-and-yet-it-works/](https://wasteofserver.com/interactive-brokers-tws-gateway-api-and-yet-it-works/)
* Interactive Broker's API Guide: [https://www.interactivebrokers.com/campus/trading-lessons/installing-configuring-tws-for-the-api/](https://www.interactivebrokers.com/campus/trading-lessons/installing-configuring-tws-for-the-api/)
* Troubleshooting IBKR Gateway setup via EC2: [https://groups.io/g/twsapi/message/48751](https://groups.io/g/twsapi/message/48751)
* Backup guide to IBKR Gateway setup via EC2: [https://dimon.ca/how-to-setup-ibc-and-tws-on-headless-ubuntu-in-10-minutes/](https://dimon.ca/how-to-setup-ibc-and-tws-on-headless-ubuntu-in-10-minutes/)

Notes on Databento API for security details, daily performance, and enrichment:
* Databento Docs: [https://databento.com/docs/](https://databento.com/docs/)
* Quickstart Guide to Databento's Reference API (Python): [https://databento.com/docs/quickstart/build-first-app?historical=python&live=python&reference=python](https://databento.com/docs/quickstart/build-first-app?historical=python&live=python&reference=python)
* OHLCV-1d schema provides open and close prices at 1-day intervals.
* API Example: End of day pricing and portfolio valuation [https://databento.com/docs/examples/basics-historical/eod](https://databento.com/docs/examples/basics-historical/eod)

How to send Slack messages to multiple channels in a workspace with Python:
* [https://www.datacamp.com/tutorial/how-to-send-slack-messages-with-python](https://www.datacamp.com/tutorial/how-to-send-slack-messages-with-python)

Eleven Labs
* Goal for AI voice would be to create one based off Darin De Paul [darindepaul.com](https://www.darindepaul.com) [IMDB](https://www.imdb.com/name/nm0210875/), who has voiced a number of video games as well as Secret Level's "Concord: Tale of the Implacable" (Season 1, Episode 13). OR to have the voice be Wil Wheaton [IMDB](https://www.imdb.com/name/nm0000696/) who voiced the Ready Player One audiobook and is probably my favorite voice actor of all-time.

LlamaParse
* May be helpful for translating unstructured data into LLM optimized formats (e.g. annual reports, 13fs, endowment reports): [https://www.llamaindex.ai/llamaparse](https://www.llamaindex.ai/llamaparse)

Bloom & Plastic Labs
* Plastic Labs - Open source LLM models that can tutor and teach [https://github.com/plastic-labs](https://github.com/plastic-labs)
* Example: Bloom - Subversive AI learning companion [https://bloombot.ai/#community](https://bloombot.ai/#community)

LangGraph
* Useful for setting up multi-agent workflows: [https://www.langchain.com/langgraph](https://www.langchain.com/langgraph)
* Open source Github repository: [https://github.com/langchain-ai/langgraph?tab=readme-ov-file](https://github.com/langchain-ai/langgraph?tab=readme-ov-file)

Cloudflare
* We use Cloudflare as our DNS layer. If needed we can use their Cloud Connector to setup AWS for all other infrastructure: [https://developers.cloudflare.com/rules/cloud-connector/providers/](https://developers.cloudflare.com/rules/cloud-connector/providers/)
<br/><br/>

***
<a name="database-fields"></a>
### Database Fields

**Holdings Database (updated daily, 5 mins after close via Databento - API fields used when handy)**
* ts_effective
* symbol
* issuer_name
* trading_currency
* security_type
* listing_status
* UTTYP
* security_description
* instrument_class
* incorporation_country
* listing_country
* Last Price to 4 Decimals
* 24hr % Change
* exchange
* DataBento Metadata:
	* lei
	* figi
	* figi_ticker
	* cfi
	* us_code
	* publisher_id
	* instrument_id
* IBKR:
	* shares held
	* target weight
	* current weight
	* average price
	* cost basis
	* IBKR price
	* model value
	* model % change
* Model Funds (with fund list)
* Correlation to Index
* Vol vs Peers + Index (?)
* Cumulative Dividend (?)
* Current Yield
<br/><br/>

**Models Database (updated daily, 5 mins after close)**
* Name
* Holdings (expressed in %, broken down to 2 decimal points)
	* Target Weight
	* Current Weight
	* CompanyDB Variables ‚òùüèΩ
* Currency
* Version
* Rebalance Frequency
* Management Fee
* Launched (year, e.g. 2026)
* Benchmarks
	* Symbol
	* CUSIP
	* FIGI
* Sharpe
* Correlation
* Volatility
* Bet
* Alpha
* Cumulative Dividends
* Current Yield 
* Tracking Error (2 decimals as %)
<br/><br/>

**Reporting Database (updated daily, kicked off after daily update for models and holdings completes)**
* Current Allocation (% of Pie by Model)
* Models DB ‚òùüèΩ +
	* 1 Day Return
	* 1 Week Return
	* 1 Month Return
	* Quarter Return
	* 1H/2H Return
	* Annual Return
	* Year to Date Return
* IBKR Models ‚òùüèΩ (data for each time period by model) +
	* 1 Day Return
	* 1 Week Return
	* 1 Month Return
	* Quarter Return
	* 1H/2H Return
	* Annual Return
	* Year to Date Return
